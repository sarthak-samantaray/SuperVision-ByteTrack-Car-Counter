{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1FMggEP0CmFwodvj0HBtXbL6AXBbZqRnS","authorship_tag":"ABX9TyOSagQbH19USgeFGqm+XUrK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"7606521e522f44aca4abc3f503bd87d5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_213668bdd1764949b341ac0239f97dce","IPY_MODEL_eaf7c052c1dd417080febb4dcba74ffc","IPY_MODEL_1075588d0cf341f0a2dd0fc5df297650"],"layout":"IPY_MODEL_5e95f39aefaf4376a837b1e73c300a38"}},"213668bdd1764949b341ac0239f97dce":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d6178ad252214a99ab2d8c2d81e89c6f","placeholder":"â€‹","style":"IPY_MODEL_fd2603a6173b45cea57c7cbf515ea5cf","value":" 10%"}},"eaf7c052c1dd417080febb4dcba74ffc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_a36e0061c9984b18a80d07ba422f6b31","max":4509,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dc7edf3d4eab4ff9a7c80ecd896f1515","value":432}},"1075588d0cf341f0a2dd0fc5df297650":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0e0f3bdb110a4d3c836e915ed3b3ea79","placeholder":"â€‹","style":"IPY_MODEL_4b6bf60d16bc46dfa38d64cc0efb0919","value":" 432/4509 [00:37&lt;07:31,  9.03it/s]"}},"5e95f39aefaf4376a837b1e73c300a38":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d6178ad252214a99ab2d8c2d81e89c6f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fd2603a6173b45cea57c7cbf515ea5cf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a36e0061c9984b18a80d07ba422f6b31":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc7edf3d4eab4ff9a7c80ecd896f1515":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0e0f3bdb110a4d3c836e915ed3b3ea79":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4b6bf60d16bc46dfa38d64cc0efb0919":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a4s7zCKGOpbL","executionInfo":{"status":"ok","timestamp":1682842580404,"user_tz":-330,"elapsed":30210,"user":{"displayName":"sarthak samantaray","userId":"02239945013595992252"}},"outputId":"19f1ae8b-0619-434b-9363-ebbb88ad1ceb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import os \n","os.chdir(\"/content/drive/MyDrive/AI projects/1. Car Counter\")\n","home = os.getcwd()\n","home"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"4xM46EQ4UBkS","executionInfo":{"status":"ok","timestamp":1682843948756,"user_tz":-330,"elapsed":652,"user":{"displayName":"sarthak samantaray","userId":"02239945013595992252"}},"outputId":"2d2f423e-1f91-4ba4-f3c3-e6938cbcb78d"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/AI projects/1. Car Counter'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["!pip install loguru\n","!pip install lap\n","!pip install ultralytics\n","\n","\n","!git clone https://github.com/ifzhang/ByteTrack.git\n","%cd {home}/ByteTrack\n","!sed -i 's/onnx==1.8.1/onnx==1.9.0/g' requirements.txt\n","\n","!pip3 install -q -r requirements.txt\n","!python3 setup.py -q develop\n","!pip install -q cython_bbox\n","!pip install -q onemetric\n","\n","!pip install supervision==0.1.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r6SYXO_rUMsJ","executionInfo":{"status":"ok","timestamp":1682844065217,"user_tz":-330,"elapsed":9354,"user":{"displayName":"sarthak samantaray","userId":"02239945013595992252"}},"outputId":"b45fecaf-b54d-4fe9-c195-a109a9b02025"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/49.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting supervision==0.1.0\n","  Downloading supervision-0.1.0-py3-none-any.whl (14 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from supervision==0.1.0) (1.22.4)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from supervision==0.1.0) (4.7.0.72)\n","Installing collected packages: supervision\n","Successfully installed supervision-0.1.0\n"]}]},{"cell_type":"code","source":["# Checking everything.\n","import ultralytics \n","print(ultralytics.checks())\n","\n","import supervision\n","print(f\"Supervision version = {supervision.__version__}\")\n"," \n","import yolox\n","print(f\"yolox verison = {yolox.__version__}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o4zL9ZIAUP7R","executionInfo":{"status":"ok","timestamp":1682844089489,"user_tz":-330,"elapsed":17032,"user":{"displayName":"sarthak samantaray","userId":"02239945013595992252"}},"outputId":"1ddda22d-6542-47b4-b059-686fea133674"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["Ultralytics YOLOv8.0.90 ðŸš€ Python-3.10.11 torch-2.0.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n","Setup complete âœ… (2 CPUs, 12.7 GB RAM, 23.5/78.2 GB disk)\n"]},{"output_type":"stream","name":"stdout","text":["None\n","Supervision version = 0.1.0\n","yolox verison = 0.1.0\n"]}]},{"cell_type":"code","source":["from supervision.video.source import get_video_frames_generator\n","from supervision.draw.color import ColorPalette\n","from supervision.notebook.utils import show_frame_in_notebook\n","from supervision.tools.detections import Detections,BoxAnnotator\n","from supervision.video.sink import VideoSink # To save the video.\n","from supervision.video.dataclasses import VideoInfo\n","from supervision.tools.line_counter import LineCounter, LineCounterAnnotator\n","from supervision.geometry.dataclasses import Point\n","from tqdm.notebook import tqdm\n","import numpy as np\n","from ultralytics import YOLO"],"metadata":{"id":"TP8zfD64UrRi","executionInfo":{"status":"ok","timestamp":1682844683918,"user_tz":-330,"elapsed":2,"user":{"displayName":"sarthak samantaray","userId":"02239945013595992252"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["from yolox.tracker.byte_tracker import BYTETracker, STrack\n","from onemetric.cv.utils.iou import box_iou_batch\n","from dataclasses import dataclass\n","\n","\n","@dataclass(frozen=True)\n","class BYTETrackerArgs:\n","    track_thresh: float = 0.25\n","    track_buffer: int = 30\n","    match_thresh: float = 0.8\n","    aspect_ratio_thresh: float = 3.0\n","    min_box_area: float = 1.0\n","    mot20: bool = False"],"metadata":{"id":"pX_xXgjReo1j","executionInfo":{"status":"ok","timestamp":1682846698920,"user_tz":-330,"elapsed":2,"user":{"displayName":"sarthak samantaray","userId":"02239945013595992252"}}},"execution_count":63,"outputs":[]},{"cell_type":"code","source":["from typing import List\n","\n","import numpy as np\n","\n","\n","# converts Detections into format that can be consumed by match_detections_with_tracks function\n","def detections2boxes(detections: Detections) -> np.ndarray:\n","    # This will just horizontally stack the two values, looks like this [1,2,3,4,5] , 1 to 4 are the location, 5 is the conf.\n","    return np.hstack((\n","        detections.xyxy,\n","        # It makes confidence of each object in a seperate array. if conf = [1,2,3,4] then will change it into [1],[2],[3]...\n","        detections.confidence[:, np.newaxis]\n","    ))\n","\n","\n","# converts List[STrack] into format that can be consumed by match_detections_with_tracks function\n","# This will turn the tracks in to xmin,ymin,xmax,ymax\n","def tracks2boxes(tracks: List[STrack]) -> np.ndarray:\n","    return np.array([\n","        track.tlbr\n","        for track\n","        in tracks\n","    ], dtype=float)\n","\n","\n","\n","\n","\n","\n","\n","# This function takes in a set of detections and a list of tracks and matches the detections to the \n","# corresponding tracks based on their bounding box coordinates.\n","\n","# First, it checks if there are any detections or tracks. If there are none, it returns an empty array.\n","\n","# Next, it converts the tracks to bounding boxes using the tracks2boxes function and computes \n","# the intersection over union (IoU) between each track's bounding box and each detection's bounding box using the box_iou_batch function.\n","\n","# Then, it finds the index of the detection with the highest IoU for each track using np.argmax, and stores these indices in track2detection.\n","\n","# The function then initializes an empty list called tracker_ids with the same length as the number of detections. For each track, \n","# it checks if the highest IoU between the track and the detections is not zero. If it's not zero, it stores the track's ID in tracker_ids at \n","# the index corresponding to the detection with the highest IoU.\n","\n","# Finally, the function returns the list of tracker IDs for each detection.\n","\n","# matches our bounding boxes with predictions\n","def match_detections_with_tracks(\n","    detections: Detections, \n","    tracks: List[STrack]\n",") -> Detections:\n","    if not np.any(detections.xyxy) or len(tracks) == 0:\n","        return np.empty((0,))\n","\n","    tracks_boxes = tracks2boxes(tracks=tracks)\n","    iou = box_iou_batch(tracks_boxes, detections.xyxy)\n","    track2detection = np.argmax(iou, axis=1)\n","    \n","    tracker_ids = [None] * len(detections)\n","    \n","    for tracker_index, detection_index in enumerate(track2detection):\n","        if iou[tracker_index, detection_index] != 0:\n","            tracker_ids[detection_index] = tracks[tracker_index].track_id\n","\n","    return tracker_ids"],"metadata":{"id":"RejVdnUyesLZ","executionInfo":{"status":"ok","timestamp":1682846704243,"user_tz":-330,"elapsed":2,"user":{"displayName":"sarthak samantaray","userId":"02239945013595992252"}}},"execution_count":64,"outputs":[]},{"cell_type":"code","source":["# Settings \n","LINE_START = Point(0,450)\n","LINE_END = Point(1920,450)\n","\n","TARGET_VIDEO_PATH = f\"{home}/Videos/Results101.mp4\""],"metadata":{"id":"JaSxZrXCetog","executionInfo":{"status":"ok","timestamp":1682846834661,"user_tz":-330,"elapsed":1,"user":{"displayName":"sarthak samantaray","userId":"02239945013595992252"}}},"execution_count":65,"outputs":[]},{"cell_type":"markdown","source":["# The Flow\n","1. Video Source\n","2. Model\n","3. model.fuse()\n","4. class_id (the ones you want to be detected)\n","5. class_names_dict (the names of the classes)\n","6. initialize byte tracker\n","7. video_info(takes in source path)\n","8. create generator\n","9. create line counter instance\n","10. box_annotator instance\n","11. line annotator instance\n","\n","12. with videoSink(target_video,video_info) as sink:\n","13. loop over frames\n","14. result\n","15. detections\n","16. filtering out detections with unwanted classes\n","\n","17. tracking detections\n","18. extracting tracker id\n","19. filtering out detections without trackers\n","20. Labels\n","21. updating line counter\n","22. make bbox\n","23. make line\n","24. sink.write to save the video."],"metadata":{"id":"oUPSWb4_fNig"}},{"cell_type":"code","source":["# Video Source\n","SOURCE_VIDEO_PATH = \"/content/drive/MyDrive/AI projects/1. Car Counter/Videos/videoplayback (1).mp4\"\n","\n","# Model\n","model = YOLO(f\"{home}/yolo_weights/yolov8x.pt\")\n","model.fuse()\n","\n","# Class ID\n","CLASS_ID = [2,3,5,7]\n","\n","# Class Names Dict\n","CLASS_NAMES_DICT = model.model.names\n","\n","# Create Byte Track\n","byte_tracker = BYTETracker(BYTETrackerArgs())\n","\n","# video_info\n","video_info = VideoInfo.from_video_path(SOURCE_VIDEO_PATH)\n","\n","# Generator\n","generator = get_video_frames_generator(SOURCE_VIDEO_PATH)\n","\n","# line counter\n","line_counter = LineCounter(start = LINE_START , end = LINE_END)\n","\n","# box annotator\n","box_annotator = BoxAnnotator(color = ColorPalette(), thickness = 2, text_thickness=1, text_scale=1)\n","\n","# LineCounterAnnotator\n","line_annotator = LineCounterAnnotator(thickness=2,text_thickness = 1, text_scale=2)\n","\n","# Open the target video file\n","with VideoSink(TARGET_VIDEO_PATH , video_info) as sink:\n","  # loop over the frames\n","  for frame in tqdm(generator , total = video_info.total_frames):\n","    results = model(frame)\n","    detections = Detections(\n","        xyxy = results[0].boxes.xyxy.cpu().numpy(),\n","        confidence = results[0].boxes.conf.cpu().numpy(),\n","        class_id = results[0].boxes.cls.cpu().numpy().astype(int)\n","    )\n","    # Filtering detections\n","    mask = np.array([class_id in CLASS_ID for class_id in detections.class_id])\n","    detections.filter(mask=mask,inplace=True)\n","\n","    # Tracking\n","    tracks = byte_tracker.update(\n","        output_results = detections2boxes(detections=detections),\n","        img_info = frame.shape,\n","        img_size = frame.shape\n","    )\n","    # Exatract tracking id\n","    tracker_id = match_detections_with_tracks(detections=detections , tracks = tracks)\n","    detections.tracker_id = np.array(tracker_id)\n","\n","    # Filtering trackers\n","    mask = np.array([tracker_id is not None for tracker_id in detections.tracker_id])\n","    detections.filter(mask=mask ,inplace=True)\n","\n","    # Format Custom labels\n","    labels = [\n","        f\"#{tracker_id} {CLASS_NAMES_DICT[class_id]},{confidence:0.3f}\"\n","        for _, confidence,class_id,tracker_id in detections\n","    ]\n","    # updating line counter\n","    line_counter.update(detections=detections)\n","    # annotate and display frame\n","    frame = box_annotator.annotate(frame=frame , detections=detections , labels=labels)\n","    line_annotator.annotate(frame=frame, line_counter=line_counter)\n","    sink.write_frame(frame)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["7606521e522f44aca4abc3f503bd87d5","213668bdd1764949b341ac0239f97dce","eaf7c052c1dd417080febb4dcba74ffc","1075588d0cf341f0a2dd0fc5df297650","5e95f39aefaf4376a837b1e73c300a38","d6178ad252214a99ab2d8c2d81e89c6f","fd2603a6173b45cea57c7cbf515ea5cf","a36e0061c9984b18a80d07ba422f6b31","dc7edf3d4eab4ff9a7c80ecd896f1515","0e0f3bdb110a4d3c836e915ed3b3ea79","4b6bf60d16bc46dfa38d64cc0efb0919"]},"id":"h0SR6Xe4foa5","executionInfo":{"status":"error","timestamp":1682848903760,"user_tz":-330,"elapsed":41204,"user":{"displayName":"sarthak samantaray","userId":"02239945013595992252"}},"outputId":"b29f9e04-7494-45f9-ab35-a70eff820536"},"execution_count":70,"outputs":[{"output_type":"stream","name":"stderr","text":["YOLOv8x summary (fused): 268 layers, 68200608 parameters, 0 gradients, 257.8 GFLOPs\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/4509 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7606521e522f44aca4abc3f503bd87d5"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\n","0: 384x640 11 cars, 1 truck, 65.0ms\n","Speed: 2.5ms preprocess, 65.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 39.9ms\n","Speed: 3.0ms preprocess, 39.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 37.3ms\n","Speed: 2.7ms preprocess, 37.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 1 truck, 36.0ms\n","Speed: 3.2ms preprocess, 36.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 1 truck, 35.6ms\n","Speed: 3.5ms preprocess, 35.6ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 1 truck, 34.5ms\n","Speed: 3.5ms preprocess, 34.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 person, 9 cars, 1 truck, 31.4ms\n","Speed: 2.5ms preprocess, 31.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 31.5ms\n","Speed: 3.0ms preprocess, 31.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 1 truck, 31.4ms\n","Speed: 3.2ms preprocess, 31.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 31.7ms\n","Speed: 3.2ms preprocess, 31.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 1 truck, 31.7ms\n","Speed: 5.4ms preprocess, 31.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 12 cars, 1 truck, 31.6ms\n","Speed: 3.3ms preprocess, 31.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 13 cars, 1 truck, 31.6ms\n","Speed: 2.8ms preprocess, 31.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 13 cars, 31.8ms\n","Speed: 5.0ms preprocess, 31.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 1 truck, 30.5ms\n","Speed: 3.7ms preprocess, 30.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 13 cars, 30.2ms\n","Speed: 10.5ms preprocess, 30.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 1 truck, 30.5ms\n","Speed: 3.5ms preprocess, 30.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 1 truck, 31.7ms\n","Speed: 3.1ms preprocess, 31.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 1 truck, 31.9ms\n","Speed: 3.1ms preprocess, 31.9ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 12 cars, 32.4ms\n","Speed: 3.1ms preprocess, 32.4ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 32.8ms\n","Speed: 3.0ms preprocess, 32.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 1 truck, 32.3ms\n","Speed: 3.1ms preprocess, 32.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 1 truck, 31.6ms\n","Speed: 3.6ms preprocess, 31.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 31.5ms\n","Speed: 3.0ms preprocess, 31.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 1 truck, 31.4ms\n","Speed: 3.2ms preprocess, 31.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 29.5ms\n","Speed: 3.3ms preprocess, 29.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 1 truck, 30.1ms\n","Speed: 4.0ms preprocess, 30.1ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 12 cars, 31.0ms\n","Speed: 5.7ms preprocess, 31.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 1 truck, 30.1ms\n","Speed: 2.9ms preprocess, 30.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 2 trucks, 30.7ms\n","Speed: 3.0ms preprocess, 30.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 2 trucks, 30.2ms\n","Speed: 2.9ms preprocess, 30.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 3 trucks, 30.5ms\n","Speed: 3.0ms preprocess, 30.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 2 trucks, 29.8ms\n","Speed: 3.7ms preprocess, 29.8ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 2 trucks, 30.6ms\n","Speed: 3.7ms preprocess, 30.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 1 truck, 30.2ms\n","Speed: 3.1ms preprocess, 30.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 1 truck, 30.6ms\n","Speed: 3.3ms preprocess, 30.6ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 1 truck, 30.1ms\n","Speed: 2.1ms preprocess, 30.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 1 truck, 30.6ms\n","Speed: 2.5ms preprocess, 30.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 1 truck, 29.7ms\n","Speed: 2.6ms preprocess, 29.7ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 1 truck, 30.5ms\n","Speed: 5.8ms preprocess, 30.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 1 truck, 30.7ms\n","Speed: 3.5ms preprocess, 30.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 30.0ms\n","Speed: 3.0ms preprocess, 30.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 31.7ms\n","Speed: 3.9ms preprocess, 31.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 30.9ms\n","Speed: 3.0ms preprocess, 30.9ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 32.1ms\n","Speed: 2.9ms preprocess, 32.1ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 31.0ms\n","Speed: 3.3ms preprocess, 31.0ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 1 truck, 29.6ms\n","Speed: 4.8ms preprocess, 29.6ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 2 trucks, 30.6ms\n","Speed: 3.1ms preprocess, 30.6ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 1 truck, 29.7ms\n","Speed: 4.6ms preprocess, 29.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 2 trucks, 30.5ms\n","Speed: 3.3ms preprocess, 30.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 3 trucks, 29.6ms\n","Speed: 2.0ms preprocess, 29.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 3 trucks, 32.5ms\n","Speed: 2.9ms preprocess, 32.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 3 trucks, 30.1ms\n","Speed: 3.0ms preprocess, 30.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 2 trucks, 30.9ms\n","Speed: 2.6ms preprocess, 30.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 2 trucks, 31.0ms\n","Speed: 3.2ms preprocess, 31.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 2 trucks, 30.2ms\n","Speed: 3.3ms preprocess, 30.2ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 2 trucks, 30.6ms\n","Speed: 4.0ms preprocess, 30.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 3 trucks, 30.8ms\n","Speed: 3.5ms preprocess, 30.8ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 2 trucks, 30.1ms\n","Speed: 3.6ms preprocess, 30.1ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 2 trucks, 31.5ms\n","Speed: 5.0ms preprocess, 31.5ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 3 trucks, 32.8ms\n","Speed: 4.3ms preprocess, 32.8ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 3 trucks, 31.0ms\n","Speed: 2.8ms preprocess, 31.0ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 1 airplane, 1 truck, 30.5ms\n","Speed: 9.9ms preprocess, 30.5ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 3 trucks, 30.6ms\n","Speed: 3.4ms preprocess, 30.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 2 trucks, 29.9ms\n","Speed: 3.5ms preprocess, 29.9ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 2 trucks, 29.9ms\n","Speed: 3.3ms preprocess, 29.9ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 2 trucks, 33.0ms\n","Speed: 2.8ms preprocess, 33.0ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 1 truck, 30.5ms\n","Speed: 3.5ms preprocess, 30.5ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 7 cars, 4 trucks, 30.6ms\n","Speed: 5.2ms preprocess, 30.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 3 trucks, 30.6ms\n","Speed: 3.2ms preprocess, 30.6ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 7 cars, 4 trucks, 32.0ms\n","Speed: 7.1ms preprocess, 32.0ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 3 trucks, 29.8ms\n","Speed: 4.4ms preprocess, 29.8ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 3 trucks, 30.3ms\n","Speed: 3.3ms preprocess, 30.3ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 3 trucks, 30.6ms\n","Speed: 2.9ms preprocess, 30.6ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 4 trucks, 29.8ms\n","Speed: 2.8ms preprocess, 29.8ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 7 cars, 6 trucks, 29.9ms\n","Speed: 2.9ms preprocess, 29.9ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 5 trucks, 30.6ms\n","Speed: 3.4ms preprocess, 30.6ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 7 cars, 6 trucks, 31.4ms\n","Speed: 3.0ms preprocess, 31.4ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 4 trucks, 30.6ms\n","Speed: 3.0ms preprocess, 30.6ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 7 cars, 7 trucks, 30.6ms\n","Speed: 2.9ms preprocess, 30.6ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 5 trucks, 32.7ms\n","Speed: 4.9ms preprocess, 32.7ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 5 trucks, 30.6ms\n","Speed: 5.8ms preprocess, 30.6ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 2 trucks, 31.2ms\n","Speed: 6.0ms preprocess, 31.2ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 3 trucks, 30.7ms\n","Speed: 3.0ms preprocess, 30.7ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 2 trucks, 31.2ms\n","Speed: 3.4ms preprocess, 31.2ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 12 cars, 2 trucks, 31.2ms\n","Speed: 2.9ms preprocess, 31.2ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 3 trucks, 31.7ms\n","Speed: 3.0ms preprocess, 31.7ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 13 cars, 1 truck, 31.3ms\n","Speed: 5.2ms preprocess, 31.3ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 1 truck, 30.8ms\n","Speed: 3.7ms preprocess, 30.8ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 13 cars, 1 truck, 31.0ms\n","Speed: 2.8ms preprocess, 31.0ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 3 trucks, 33.5ms\n","Speed: 3.5ms preprocess, 33.5ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 1 truck, 31.2ms\n","Speed: 4.6ms preprocess, 31.2ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 12 cars, 31.1ms\n","Speed: 3.1ms preprocess, 31.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 2 trucks, 31.1ms\n","Speed: 3.2ms preprocess, 31.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 1 truck, 31.0ms\n","Speed: 4.1ms preprocess, 31.0ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 2 trucks, 31.0ms\n","Speed: 2.8ms preprocess, 31.0ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 3 trucks, 31.1ms\n","Speed: 3.2ms preprocess, 31.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 2 trucks, 30.4ms\n","Speed: 3.0ms preprocess, 30.4ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 13 cars, 30.8ms\n","Speed: 3.7ms preprocess, 30.8ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 13 cars, 30.1ms\n","Speed: 2.9ms preprocess, 30.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 13 cars, 34.7ms\n","Speed: 2.7ms preprocess, 34.7ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 14 cars, 1 truck, 30.2ms\n","Speed: 2.8ms preprocess, 30.2ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 14 cars, 1 truck, 31.6ms\n","Speed: 2.8ms preprocess, 31.6ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 14 cars, 4 trucks, 30.8ms\n","Speed: 4.9ms preprocess, 30.8ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 3 trucks, 30.6ms\n","Speed: 2.9ms preprocess, 30.6ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 14 cars, 3 trucks, 31.0ms\n","Speed: 2.9ms preprocess, 31.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 14 cars, 4 trucks, 31.7ms\n","Speed: 2.7ms preprocess, 31.7ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 13 cars, 4 trucks, 31.4ms\n","Speed: 3.1ms preprocess, 31.4ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 cars, 3 trucks, 30.2ms\n","Speed: 2.8ms preprocess, 30.2ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 14 cars, 2 trucks, 30.1ms\n","Speed: 2.9ms preprocess, 30.1ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 14 cars, 3 trucks, 30.1ms\n","Speed: 3.2ms preprocess, 30.1ms inference, 6.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 4 trucks, 30.4ms\n","Speed: 2.5ms preprocess, 30.4ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 3 trucks, 30.1ms\n","Speed: 2.4ms preprocess, 30.1ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 13 cars, 1 truck, 31.1ms\n","Speed: 2.9ms preprocess, 31.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 13 cars, 1 truck, 30.3ms\n","Speed: 3.4ms preprocess, 30.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 12 cars, 2 trucks, 30.7ms\n","Speed: 3.7ms preprocess, 30.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 13 cars, 1 truck, 29.9ms\n","Speed: 3.3ms preprocess, 29.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 12 cars, 1 truck, 30.9ms\n","Speed: 2.5ms preprocess, 30.9ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 cars, 1 truck, 34.1ms\n","Speed: 4.1ms preprocess, 34.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 1 person, 12 cars, 2 trucks, 31.3ms\n","Speed: 2.6ms preprocess, 31.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 12 cars, 1 truck, 30.1ms\n","Speed: 8.7ms preprocess, 30.1ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 2 trucks, 29.6ms\n","Speed: 2.7ms preprocess, 29.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 2 trucks, 31.4ms\n","Speed: 3.0ms preprocess, 31.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 13 cars, 1 truck, 30.8ms\n","Speed: 3.2ms preprocess, 30.8ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 12 cars, 2 trucks, 30.6ms\n","Speed: 4.2ms preprocess, 30.6ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 14 cars, 1 truck, 31.1ms\n","Speed: 3.2ms preprocess, 31.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 1 truck, 30.4ms\n","Speed: 3.0ms preprocess, 30.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 2 trucks, 30.9ms\n","Speed: 3.1ms preprocess, 30.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 2 trucks, 31.9ms\n","Speed: 8.5ms preprocess, 31.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 3 trucks, 32.5ms\n","Speed: 3.1ms preprocess, 32.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 2 trucks, 32.2ms\n","Speed: 3.5ms preprocess, 32.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 12 cars, 1 airplane, 2 trucks, 32.3ms\n","Speed: 3.0ms preprocess, 32.3ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 3 trucks, 32.4ms\n","Speed: 3.3ms preprocess, 32.4ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 3 trucks, 29.5ms\n","Speed: 3.1ms preprocess, 29.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 12 cars, 3 trucks, 31.0ms\n","Speed: 3.1ms preprocess, 31.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 13 cars, 1 truck, 31.1ms\n","Speed: 10.6ms preprocess, 31.1ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 2 trucks, 30.3ms\n","Speed: 3.1ms preprocess, 30.3ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 12 cars, 3 trucks, 29.7ms\n","Speed: 3.4ms preprocess, 29.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 13 cars, 30.0ms\n","Speed: 3.2ms preprocess, 30.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 12 cars, 1 truck, 31.1ms\n","Speed: 2.7ms preprocess, 31.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 13 cars, 30.3ms\n","Speed: 3.7ms preprocess, 30.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 13 cars, 31.1ms\n","Speed: 3.4ms preprocess, 31.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 12 cars, 1 truck, 29.8ms\n","Speed: 2.5ms preprocess, 29.8ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 12 cars, 2 trucks, 30.9ms\n","Speed: 2.6ms preprocess, 30.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 14 cars, 30.1ms\n","Speed: 2.8ms preprocess, 30.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 12 cars, 1 truck, 31.5ms\n","Speed: 2.9ms preprocess, 31.5ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 13 cars, 1 truck, 31.6ms\n","Speed: 3.1ms preprocess, 31.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 14 cars, 1 truck, 30.2ms\n","Speed: 2.4ms preprocess, 30.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 13 cars, 1 truck, 31.0ms\n","Speed: 2.9ms preprocess, 31.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 14 cars, 1 truck, 30.4ms\n","Speed: 2.9ms preprocess, 30.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 15 cars, 1 truck, 30.7ms\n","Speed: 2.2ms preprocess, 30.7ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 14 cars, 3 trucks, 30.2ms\n","Speed: 3.1ms preprocess, 30.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 4 trucks, 31.2ms\n","Speed: 3.1ms preprocess, 31.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 12 cars, 4 trucks, 30.5ms\n","Speed: 2.8ms preprocess, 30.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 5 trucks, 31.1ms\n","Speed: 3.4ms preprocess, 31.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 3 trucks, 32.1ms\n","Speed: 3.5ms preprocess, 32.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 12 cars, 1 truck, 31.8ms\n","Speed: 2.1ms preprocess, 31.8ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 1 truck, 32.0ms\n","Speed: 4.4ms preprocess, 32.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 4 trucks, 32.2ms\n","Speed: 3.4ms preprocess, 32.2ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 4 trucks, 32.2ms\n","Speed: 4.7ms preprocess, 32.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 12 cars, 3 trucks, 33.6ms\n","Speed: 12.2ms preprocess, 33.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 6 trucks, 43.4ms\n","Speed: 5.6ms preprocess, 43.4ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 6 trucks, 38.0ms\n","Speed: 3.0ms preprocess, 38.0ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 13 cars, 4 trucks, 30.3ms\n","Speed: 2.9ms preprocess, 30.3ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 12 cars, 3 trucks, 29.2ms\n","Speed: 3.3ms preprocess, 29.2ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 12 cars, 4 trucks, 31.3ms\n","Speed: 3.2ms preprocess, 31.3ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 5 trucks, 30.8ms\n","Speed: 2.7ms preprocess, 30.8ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 3 trucks, 31.1ms\n","Speed: 2.4ms preprocess, 31.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 4 trucks, 32.5ms\n","Speed: 4.4ms preprocess, 32.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 5 trucks, 32.7ms\n","Speed: 2.8ms preprocess, 32.7ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 12 cars, 2 trucks, 32.4ms\n","Speed: 3.0ms preprocess, 32.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 2 trucks, 54.3ms\n","Speed: 2.9ms preprocess, 54.3ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 5 trucks, 47.1ms\n","Speed: 3.0ms preprocess, 47.1ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 3 trucks, 32.5ms\n","Speed: 3.9ms preprocess, 32.5ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 4 trucks, 32.8ms\n","Speed: 2.6ms preprocess, 32.8ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 4 trucks, 32.5ms\n","Speed: 3.2ms preprocess, 32.5ms inference, 2.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 2 trucks, 32.4ms\n","Speed: 3.1ms preprocess, 32.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 3 trucks, 30.6ms\n","Speed: 6.2ms preprocess, 30.6ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 3 trucks, 30.9ms\n","Speed: 3.0ms preprocess, 30.9ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 29.9ms\n","Speed: 5.8ms preprocess, 29.9ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 1 truck, 30.9ms\n","Speed: 3.0ms preprocess, 30.9ms inference, 7.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 2 airplanes, 1 truck, 34.3ms\n","Speed: 3.0ms preprocess, 34.3ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 1 truck, 37.4ms\n","Speed: 3.0ms preprocess, 37.4ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 31.5ms\n","Speed: 3.1ms preprocess, 31.5ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 1 airplane, 30.6ms\n","Speed: 3.3ms preprocess, 30.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 1 airplane, 1 truck, 30.3ms\n","Speed: 2.9ms preprocess, 30.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 1 airplane, 1 truck, 30.8ms\n","Speed: 6.7ms preprocess, 30.8ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 3 trucks, 30.4ms\n","Speed: 3.9ms preprocess, 30.4ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 2 trucks, 31.4ms\n","Speed: 2.9ms preprocess, 31.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 1 truck, 30.4ms\n","Speed: 3.0ms preprocess, 30.4ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 2 trucks, 30.0ms\n","Speed: 3.8ms preprocess, 30.0ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 31.2ms\n","Speed: 2.8ms preprocess, 31.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 2 trucks, 30.7ms\n","Speed: 2.1ms preprocess, 30.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 3 trucks, 29.9ms\n","Speed: 7.7ms preprocess, 29.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 2 trucks, 30.7ms\n","Speed: 4.4ms preprocess, 30.7ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 4 trucks, 31.0ms\n","Speed: 6.9ms preprocess, 31.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 2 trucks, 31.6ms\n","Speed: 5.4ms preprocess, 31.6ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 2 trucks, 30.3ms\n","Speed: 6.1ms preprocess, 30.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 2 trucks, 34.7ms\n","Speed: 2.6ms preprocess, 34.7ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 2 trucks, 30.9ms\n","Speed: 2.9ms preprocess, 30.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 1 airplane, 3 trucks, 30.8ms\n","Speed: 3.7ms preprocess, 30.8ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 2 trucks, 30.7ms\n","Speed: 2.9ms preprocess, 30.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 2 trucks, 31.1ms\n","Speed: 2.8ms preprocess, 31.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 3 trucks, 30.3ms\n","Speed: 3.2ms preprocess, 30.3ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 7 cars, 1 truck, 30.7ms\n","Speed: 7.9ms preprocess, 30.7ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 1 truck, 31.2ms\n","Speed: 3.1ms preprocess, 31.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 1 truck, 29.9ms\n","Speed: 2.8ms preprocess, 29.9ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 1 truck, 30.7ms\n","Speed: 3.6ms preprocess, 30.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 30.8ms\n","Speed: 2.8ms preprocess, 30.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 6 cars, 31.3ms\n","Speed: 3.0ms preprocess, 31.3ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 6 cars, 2 trucks, 31.4ms\n","Speed: 3.4ms preprocess, 31.4ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 7 cars, 1 truck, 30.4ms\n","Speed: 2.9ms preprocess, 30.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 7 cars, 1 airplane, 2 trucks, 31.1ms\n","Speed: 2.9ms preprocess, 31.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 6 cars, 1 airplane, 1 truck, 30.5ms\n","Speed: 2.9ms preprocess, 30.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 1 truck, 31.2ms\n","Speed: 3.4ms preprocess, 31.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 7 cars, 1 truck, 31.0ms\n","Speed: 2.9ms preprocess, 31.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 7 cars, 1 truck, 30.2ms\n","Speed: 3.4ms preprocess, 30.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 6 cars, 3 trucks, 31.5ms\n","Speed: 6.4ms preprocess, 31.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 6 cars, 2 trucks, 30.6ms\n","Speed: 4.4ms preprocess, 30.6ms inference, 2.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 7 cars, 2 trucks, 30.7ms\n","Speed: 2.9ms preprocess, 30.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 7 cars, 2 trucks, 30.9ms\n","Speed: 3.1ms preprocess, 30.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 2 airplanes, 1 truck, 30.0ms\n","Speed: 3.3ms preprocess, 30.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 1 airplane, 1 truck, 31.2ms\n","Speed: 2.9ms preprocess, 31.2ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 3 airplanes, 3 trucks, 31.9ms\n","Speed: 3.5ms preprocess, 31.9ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 2 trucks, 31.6ms\n","Speed: 7.2ms preprocess, 31.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 2 airplanes, 3 trucks, 31.9ms\n","Speed: 3.1ms preprocess, 31.9ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 3 trucks, 32.0ms\n","Speed: 2.9ms preprocess, 32.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 5 cars, 2 airplanes, 4 trucks, 31.5ms\n","Speed: 2.9ms preprocess, 31.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 2 trucks, 30.9ms\n","Speed: 2.9ms preprocess, 30.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 1 truck, 30.7ms\n","Speed: 3.3ms preprocess, 30.7ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 1 truck, 30.8ms\n","Speed: 3.7ms preprocess, 30.8ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 1 truck, 31.3ms\n","Speed: 3.0ms preprocess, 31.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 1 truck, 31.0ms\n","Speed: 2.8ms preprocess, 31.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 6 cars, 2 airplanes, 3 trucks, 31.2ms\n","Speed: 3.3ms preprocess, 31.2ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 7 cars, 1 airplane, 1 truck, 31.6ms\n","Speed: 2.7ms preprocess, 31.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 5 cars, 2 trucks, 31.2ms\n","Speed: 3.2ms preprocess, 31.2ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 6 cars, 1 truck, 30.2ms\n","Speed: 3.4ms preprocess, 30.2ms inference, 4.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 7 cars, 29.6ms\n","Speed: 3.0ms preprocess, 29.6ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 6 cars, 1 truck, 30.3ms\n","Speed: 4.0ms preprocess, 30.3ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 6 cars, 1 truck, 30.0ms\n","Speed: 2.7ms preprocess, 30.0ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 6 cars, 1 truck, 30.0ms\n","Speed: 2.8ms preprocess, 30.0ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 34.0ms\n","Speed: 2.9ms preprocess, 34.0ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 7 cars, 29.9ms\n","Speed: 2.8ms preprocess, 29.9ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 1 truck, 30.5ms\n","Speed: 2.9ms preprocess, 30.5ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 30.2ms\n","Speed: 2.8ms preprocess, 30.2ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 5 cars, 3 trucks, 30.8ms\n","Speed: 3.0ms preprocess, 30.8ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 4 cars, 5 trucks, 30.7ms\n","Speed: 3.0ms preprocess, 30.7ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 6 cars, 1 truck, 30.2ms\n","Speed: 4.4ms preprocess, 30.2ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 7 cars, 2 trucks, 31.1ms\n","Speed: 3.5ms preprocess, 31.1ms inference, 2.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 7 cars, 1 truck, 30.9ms\n","Speed: 4.9ms preprocess, 30.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 7 cars, 1 truck, 30.7ms\n","Speed: 3.0ms preprocess, 30.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 6 cars, 2 trucks, 30.0ms\n","Speed: 3.1ms preprocess, 30.0ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 4 cars, 3 trucks, 30.7ms\n","Speed: 2.8ms preprocess, 30.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 1 truck, 34.3ms\n","Speed: 3.1ms preprocess, 34.3ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 2 trucks, 30.3ms\n","Speed: 2.8ms preprocess, 30.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 2 trucks, 31.3ms\n","Speed: 2.9ms preprocess, 31.3ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 2 trucks, 30.8ms\n","Speed: 3.2ms preprocess, 30.8ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 3 trucks, 31.9ms\n","Speed: 2.9ms preprocess, 31.9ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 5 cars, 2 trucks, 30.6ms\n","Speed: 3.0ms preprocess, 30.6ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 7 cars, 2 trucks, 31.0ms\n","Speed: 2.7ms preprocess, 31.0ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 2 trucks, 31.5ms\n","Speed: 3.0ms preprocess, 31.5ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 1 truck, 35.0ms\n","Speed: 3.0ms preprocess, 35.0ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 4 cars, 3 trucks, 32.3ms\n","Speed: 3.4ms preprocess, 32.3ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 6 cars, 3 trucks, 32.0ms\n","Speed: 2.8ms preprocess, 32.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 6 cars, 2 trucks, 34.0ms\n","Speed: 4.5ms preprocess, 34.0ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 6 cars, 2 trucks, 32.3ms\n","Speed: 2.9ms preprocess, 32.3ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 7 cars, 2 trucks, 32.4ms\n","Speed: 3.0ms preprocess, 32.4ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 7 cars, 3 trucks, 32.3ms\n","Speed: 3.0ms preprocess, 32.3ms inference, 12.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 7 cars, 2 trucks, 32.6ms\n","Speed: 4.0ms preprocess, 32.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 7 cars, 2 trucks, 31.9ms\n","Speed: 4.4ms preprocess, 31.9ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 5 cars, 3 trucks, 32.1ms\n","Speed: 3.0ms preprocess, 32.1ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 7 cars, 1 truck, 33.4ms\n","Speed: 3.1ms preprocess, 33.4ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 2 trucks, 32.4ms\n","Speed: 2.9ms preprocess, 32.4ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 12 cars, 1 truck, 32.5ms\n","Speed: 3.0ms preprocess, 32.5ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 12 cars, 1 truck, 32.8ms\n","Speed: 2.9ms preprocess, 32.8ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 1 truck, 32.8ms\n","Speed: 3.0ms preprocess, 32.8ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 1 truck, 32.6ms\n","Speed: 2.8ms preprocess, 32.6ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 13 cars, 2 trucks, 32.4ms\n","Speed: 3.0ms preprocess, 32.4ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 1 truck, 32.4ms\n","Speed: 3.1ms preprocess, 32.4ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 1 truck, 32.9ms\n","Speed: 2.7ms preprocess, 32.9ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 2 trucks, 32.9ms\n","Speed: 6.2ms preprocess, 32.9ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 2 trucks, 32.4ms\n","Speed: 2.8ms preprocess, 32.4ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 2 trucks, 35.5ms\n","Speed: 2.9ms preprocess, 35.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 2 trucks, 32.4ms\n","Speed: 3.0ms preprocess, 32.4ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 2 trucks, 32.6ms\n","Speed: 3.3ms preprocess, 32.6ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 2 trucks, 32.6ms\n","Speed: 3.3ms preprocess, 32.6ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 1 truck, 32.6ms\n","Speed: 3.9ms preprocess, 32.6ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 1 truck, 32.6ms\n","Speed: 2.9ms preprocess, 32.6ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 1 truck, 32.9ms\n","Speed: 3.5ms preprocess, 32.9ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 1 truck, 35.5ms\n","Speed: 2.9ms preprocess, 35.5ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 1 truck, 34.5ms\n","Speed: 2.8ms preprocess, 34.5ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 1 truck, 32.9ms\n","Speed: 2.9ms preprocess, 32.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 2 trucks, 32.9ms\n","Speed: 2.9ms preprocess, 32.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 2 trucks, 32.4ms\n","Speed: 3.1ms preprocess, 32.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 3 trucks, 32.4ms\n","Speed: 2.2ms preprocess, 32.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 2 trucks, 31.3ms\n","Speed: 3.1ms preprocess, 31.3ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 2 trucks, 29.7ms\n","Speed: 2.8ms preprocess, 29.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 2 trucks, 30.6ms\n","Speed: 2.9ms preprocess, 30.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 12 cars, 1 truck, 31.1ms\n","Speed: 2.9ms preprocess, 31.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 2 trucks, 31.7ms\n","Speed: 3.4ms preprocess, 31.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 2 trucks, 31.2ms\n","Speed: 2.9ms preprocess, 31.2ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 1 truck, 31.3ms\n","Speed: 2.8ms preprocess, 31.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 1 truck, 30.8ms\n","Speed: 3.2ms preprocess, 30.8ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 2 trucks, 35.3ms\n","Speed: 2.9ms preprocess, 35.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 3 trucks, 31.9ms\n","Speed: 4.3ms preprocess, 31.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 1 truck, 32.1ms\n","Speed: 3.1ms preprocess, 32.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 1 airplane, 2 trucks, 33.1ms\n","Speed: 3.3ms preprocess, 33.1ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 3 trucks, 31.3ms\n","Speed: 3.7ms preprocess, 31.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 7 cars, 3 trucks, 30.8ms\n","Speed: 2.3ms preprocess, 30.8ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 1 truck, 31.0ms\n","Speed: 2.3ms preprocess, 31.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 1 truck, 31.2ms\n","Speed: 9.6ms preprocess, 31.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 2 trucks, 31.4ms\n","Speed: 3.6ms preprocess, 31.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 2 trucks, 35.3ms\n","Speed: 3.0ms preprocess, 35.3ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 1 truck, 32.7ms\n","Speed: 2.9ms preprocess, 32.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 1 truck, 32.9ms\n","Speed: 3.8ms preprocess, 32.9ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 1 truck, 32.2ms\n","Speed: 3.4ms preprocess, 32.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 1 truck, 32.8ms\n","Speed: 3.4ms preprocess, 32.8ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 1 airplane, 2 trucks, 30.1ms\n","Speed: 8.6ms preprocess, 30.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 1 truck, 30.8ms\n","Speed: 3.0ms preprocess, 30.8ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 1 truck, 30.9ms\n","Speed: 7.0ms preprocess, 30.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 2 trucks, 30.3ms\n","Speed: 2.8ms preprocess, 30.3ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 1 truck, 31.1ms\n","Speed: 3.1ms preprocess, 31.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 1 truck, 31.3ms\n","Speed: 4.4ms preprocess, 31.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 1 truck, 30.0ms\n","Speed: 4.6ms preprocess, 30.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 2 trucks, 31.1ms\n","Speed: 5.2ms preprocess, 31.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 7 cars, 2 trucks, 30.9ms\n","Speed: 2.9ms preprocess, 30.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 7 cars, 1 truck, 29.8ms\n","Speed: 4.0ms preprocess, 29.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 1 truck, 31.7ms\n","Speed: 2.9ms preprocess, 31.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 1 truck, 33.0ms\n","Speed: 3.3ms preprocess, 33.0ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 2 trucks, 31.3ms\n","Speed: 3.0ms preprocess, 31.3ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 1 truck, 31.1ms\n","Speed: 3.7ms preprocess, 31.1ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 2 trucks, 31.7ms\n","Speed: 7.1ms preprocess, 31.7ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 1 truck, 32.2ms\n","Speed: 2.9ms preprocess, 32.2ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 1 truck, 31.1ms\n","Speed: 3.6ms preprocess, 31.1ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 1 truck, 31.9ms\n","Speed: 3.0ms preprocess, 31.9ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 1 truck, 31.4ms\n","Speed: 3.0ms preprocess, 31.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 1 truck, 32.1ms\n","Speed: 3.3ms preprocess, 32.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 1 truck, 30.6ms\n","Speed: 2.9ms preprocess, 30.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 7 cars, 3 trucks, 30.3ms\n","Speed: 2.4ms preprocess, 30.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 2 trucks, 31.3ms\n","Speed: 8.4ms preprocess, 31.3ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 2 trucks, 31.4ms\n","Speed: 3.3ms preprocess, 31.4ms inference, 3.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 4 trucks, 30.6ms\n","Speed: 3.4ms preprocess, 30.6ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 3 trucks, 30.2ms\n","Speed: 3.0ms preprocess, 30.2ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 2 trucks, 30.6ms\n","Speed: 3.2ms preprocess, 30.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 2 trucks, 30.2ms\n","Speed: 4.1ms preprocess, 30.2ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 2 trucks, 32.4ms\n","Speed: 5.1ms preprocess, 32.4ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 2 trucks, 29.8ms\n","Speed: 3.6ms preprocess, 29.8ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 7 cars, 4 trucks, 31.3ms\n","Speed: 3.6ms preprocess, 31.3ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 3 trucks, 30.4ms\n","Speed: 3.0ms preprocess, 30.4ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 3 trucks, 31.2ms\n","Speed: 3.3ms preprocess, 31.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 7 cars, 5 trucks, 29.7ms\n","Speed: 3.0ms preprocess, 29.7ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 7 cars, 4 trucks, 29.5ms\n","Speed: 3.1ms preprocess, 29.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 3 trucks, 30.8ms\n","Speed: 3.6ms preprocess, 30.8ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 3 trucks, 30.9ms\n","Speed: 7.8ms preprocess, 30.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 3 trucks, 30.1ms\n","Speed: 2.9ms preprocess, 30.1ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 1 bus, 3 trucks, 31.1ms\n","Speed: 3.0ms preprocess, 31.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 7 cars, 5 trucks, 32.2ms\n","Speed: 2.9ms preprocess, 32.2ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 7 cars, 4 trucks, 31.2ms\n","Speed: 2.8ms preprocess, 31.2ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 7 cars, 2 trucks, 30.2ms\n","Speed: 2.7ms preprocess, 30.2ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 7 cars, 3 trucks, 31.4ms\n","Speed: 3.2ms preprocess, 31.4ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 6 cars, 4 trucks, 31.4ms\n","Speed: 3.0ms preprocess, 31.4ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 7 cars, 3 trucks, 31.5ms\n","Speed: 3.2ms preprocess, 31.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 7 cars, 1 airplane, 3 trucks, 31.5ms\n","Speed: 3.1ms preprocess, 31.5ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 7 cars, 3 trucks, 31.2ms\n","Speed: 2.8ms preprocess, 31.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 2 trucks, 31.2ms\n","Speed: 2.7ms preprocess, 31.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 1 airplane, 2 trucks, 31.1ms\n","Speed: 3.0ms preprocess, 31.1ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 2 trucks, 29.7ms\n","Speed: 3.0ms preprocess, 29.7ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 2 trucks, 30.8ms\n","Speed: 2.9ms preprocess, 30.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 1 truck, 31.4ms\n","Speed: 3.0ms preprocess, 31.4ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 9 cars, 2 trucks, 30.5ms\n","Speed: 3.1ms preprocess, 30.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 3 trucks, 30.2ms\n","Speed: 6.0ms preprocess, 30.2ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 3 trucks, 31.5ms\n","Speed: 2.7ms preprocess, 31.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 2 trucks, 32.0ms\n","Speed: 2.8ms preprocess, 32.0ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 3 trucks, 31.4ms\n","Speed: 2.9ms preprocess, 31.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 3 trucks, 31.0ms\n","Speed: 2.4ms preprocess, 31.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 8 cars, 2 trucks, 31.3ms\n","Speed: 3.1ms preprocess, 31.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 3 trucks, 29.7ms\n","Speed: 3.2ms preprocess, 29.7ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 2 trucks, 31.5ms\n","Speed: 3.2ms preprocess, 31.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 13 cars, 1 truck, 31.2ms\n","Speed: 2.8ms preprocess, 31.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 12 cars, 2 trucks, 30.4ms\n","Speed: 3.6ms preprocess, 30.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 2 trucks, 31.6ms\n","Speed: 4.7ms preprocess, 31.6ms inference, 3.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 1 truck, 31.0ms\n","Speed: 3.2ms preprocess, 31.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 1 truck, 29.6ms\n","Speed: 3.3ms preprocess, 29.6ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 1 truck, 31.4ms\n","Speed: 3.0ms preprocess, 31.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 1 truck, 30.9ms\n","Speed: 3.5ms preprocess, 30.9ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 13 cars, 1 truck, 32.4ms\n","Speed: 2.9ms preprocess, 32.4ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 13 cars, 1 truck, 30.5ms\n","Speed: 4.1ms preprocess, 30.5ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 12 cars, 1 truck, 31.0ms\n","Speed: 3.0ms preprocess, 31.0ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 2 trucks, 29.6ms\n","Speed: 2.8ms preprocess, 29.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 1 truck, 37.2ms\n","Speed: 2.8ms preprocess, 37.2ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 12 cars, 1 truck, 30.2ms\n","Speed: 2.9ms preprocess, 30.2ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 12 cars, 1 truck, 29.4ms\n","Speed: 2.9ms preprocess, 29.4ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 1 truck, 29.8ms\n","Speed: 9.3ms preprocess, 29.8ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 1 truck, 29.9ms\n","Speed: 3.1ms preprocess, 29.9ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 1 truck, 30.1ms\n","Speed: 2.9ms preprocess, 30.1ms inference, 6.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 1 truck, 30.1ms\n","Speed: 3.1ms preprocess, 30.1ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 1 truck, 61.8ms\n","Speed: 3.6ms preprocess, 61.8ms inference, 11.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 1 truck, 100.5ms\n","Speed: 31.6ms preprocess, 100.5ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 1 truck, 40.0ms\n","Speed: 11.1ms preprocess, 40.0ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 12 cars, 2 trucks, 41.9ms\n","Speed: 3.8ms preprocess, 41.9ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 12 cars, 2 trucks, 42.7ms\n","Speed: 5.4ms preprocess, 42.7ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 10 cars, 2 trucks, 40.7ms\n","Speed: 3.1ms preprocess, 40.7ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 12 cars, 2 trucks, 40.7ms\n","Speed: 3.4ms preprocess, 40.7ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 12 cars, 2 trucks, 39.9ms\n","Speed: 3.7ms preprocess, 39.9ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 13 cars, 1 truck, 40.1ms\n","Speed: 6.3ms preprocess, 40.1ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 13 cars, 1 truck, 40.1ms\n","Speed: 3.0ms preprocess, 40.1ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 13 cars, 1 truck, 41.3ms\n","Speed: 3.8ms preprocess, 41.3ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 13 cars, 2 trucks, 38.1ms\n","Speed: 3.0ms preprocess, 38.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 13 cars, 1 truck, 45.9ms\n","Speed: 6.2ms preprocess, 45.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 12 cars, 1 truck, 38.5ms\n","Speed: 3.2ms preprocess, 38.5ms inference, 2.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 13 cars, 2 trucks, 39.1ms\n","Speed: 2.9ms preprocess, 39.1ms inference, 2.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 14 cars, 1 truck, 37.7ms\n","Speed: 4.5ms preprocess, 37.7ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 12 cars, 1 truck, 40.2ms\n","Speed: 2.8ms preprocess, 40.2ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 1 truck, 37.6ms\n","Speed: 3.0ms preprocess, 37.6ms inference, 6.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 12 cars, 1 truck, 38.1ms\n","Speed: 2.9ms preprocess, 38.1ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 12 cars, 1 truck, 40.4ms\n","Speed: 3.0ms preprocess, 40.4ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 1 airplane, 1 truck, 42.0ms\n","Speed: 3.2ms preprocess, 42.0ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 1 truck, 39.1ms\n","Speed: 3.0ms preprocess, 39.1ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 2 trucks, 38.0ms\n","Speed: 3.1ms preprocess, 38.0ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 12 cars, 2 trucks, 37.6ms\n","Speed: 4.7ms preprocess, 37.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 2 trucks, 37.6ms\n","Speed: 3.0ms preprocess, 37.6ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 2 trucks, 37.8ms\n","Speed: 7.1ms preprocess, 37.8ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 12 cars, 1 truck, 39.5ms\n","Speed: 3.1ms preprocess, 39.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 12 cars, 40.3ms\n","Speed: 2.9ms preprocess, 40.3ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 12 cars, 1 truck, 40.1ms\n","Speed: 3.3ms preprocess, 40.1ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 13 cars, 40.0ms\n","Speed: 4.7ms preprocess, 40.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 12 cars, 1 truck, 41.1ms\n","Speed: 4.7ms preprocess, 41.1ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 13 cars, 1 truck, 41.8ms\n","Speed: 2.9ms preprocess, 41.8ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 11 cars, 1 truck, 41.0ms\n","Speed: 2.9ms preprocess, 41.0ms inference, 3.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 12 cars, 41.7ms\n","Speed: 2.8ms preprocess, 41.7ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 12 cars, 2 trucks, 41.1ms\n","Speed: 3.0ms preprocess, 41.1ms inference, 2.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 13 cars, 2 trucks, 41.3ms\n","Speed: 2.9ms preprocess, 41.3ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 384x640 12 cars, 2 trucks, 40.5ms\n","Speed: 2.9ms preprocess, 40.5ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-70-2415bb599c8b>\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbox_annotator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mframe\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mdetections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdetections\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mline_annotator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline_counter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mline_counter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0msink\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/supervision/video/sink.py\u001b[0m in \u001b[0;36mwrite_frame\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mframe\u001b[0m \u001b[0mto\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mwritten\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \"\"\"\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":[],"metadata":{"id":"YeDPGPd4mz-5"},"execution_count":null,"outputs":[]}]}